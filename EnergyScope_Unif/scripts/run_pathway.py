"""
Modified run_pathway.py with AMPL Results Caching and Cost Plotting
====================================================================

This version allows you to:
1. Run full optimization once and cache AMPL results
2. Skip optimization and load cached results for fast iteration on post-processing
3. Generate investment and operation cost plots

Usage:
------
# Full run (optimization + post-processing + plots):
python run_pathway.py

# Skip optimization, load cache, only do post-processing + plots:
python run_pathway.py --skip-optimize

# Force re-optimization even if cache exists:
python run_pathway.py --force-optimize

This saves ~2.5 hours when you need to modify result extraction/printing code!
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
from pathlib import Path
import sys
import pandas as pd
import logging
import time
import argparse
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)

# Add project path
sys.path.append('/home/pjimenez/ESMC_PTH/EnergyScope_Unif/')

# Import the simplified pathway class
from esmc_pathway import EsmcPathway
from esmc.common import bo_country_code, CSV_SEPARATOR

# Import the caching functions
from ampl_cache import save_ampl_results, load_ampl_results, check_cache_exists

# Import the year .dat file generator
from generate_year_dat_files import generate_all_year_dat_files

# ===== CONFIGURATION =====

# Define the pathway case
pathway_case = 'NZE_2015_2050'

# Years to optimize
years = ['2015', '2021', '2025', '2030', '2035', '2040', '2045', '2050']

# Define phases (transitions between years)
phases = {
    '2015_2021': {'start': 'YEAR_2015', 'stop': 'YEAR_2021'},
    '2021_2025': {'start': 'YEAR_2021', 'stop': 'YEAR_2025'},
    '2025_2030': {'start': 'YEAR_2025', 'stop': 'YEAR_2030'},
    '2030_2035': {'start': 'YEAR_2030', 'stop': 'YEAR_2035'},
    '2035_2040': {'start': 'YEAR_2035', 'stop': 'YEAR_2040'},
    '2040_2045': {'start': 'YEAR_2040', 'stop': 'YEAR_2045'},
    '2045_2050': {'start': 'YEAR_2045', 'stop': 'YEAR_2050'}
}

# Pathway-specific parameters
pathway_params = {
    'limit_LT_renovation': 0.33,
    'limit_pass_mob_changes': 0.50,
    'limit_freight_changes': 0.50,
    'limit_HT_renovation': 0.33,
    'limit_cooking_changes': 0.33,
    'limit_mech_comm_changes': 0.33,
    'limit_mech_mov_agr_changes': 0.33,
    'limit_mech_fix_agr_changes': 0.33,
    'limit_mech_min_changes': 0.33,
    'limit_mech_fish_changes': 0.33
}

# Number of typical days
nbr_tds = 16

# AMPL path (None if in PATH)
ampl_path = None  

# Constraints
gwp_limit_overall = True
re_share_primary = None
f_perc = True

# Specify which hourly results to save
# Options: 'Resources', 'Exchanges', 'Assets', 'Storage', 'Curt'
save_hourly = ['Resources', 'Exchanges', 'Assets', 'Storage', 'Curt']

# ===== HELPER FUNCTIONS =====

def check_dat_files():
    """
    Check that required .dat files exist in year directories
    """
    data_dir = Path('/home/pjimenez/ESMC_PTH/EnergyScope_Unif/Data')
    
    required_files = [
        'indep.dat',
        'reg_demands.dat',
        'reg_resources.dat',
        'reg_technologies.dat',
        'reg_exch.dat',
        'reg_misc.dat',
        'reg_storage_power_to_energy.dat'
    ]
    
    all_good = True
    
    # Check each year
    for year in years:
        year_dir = data_dir / str(year)
        if not year_dir.exists():
            print(f"âœ— Year directory not found: {year_dir}")
            all_good = False
            continue
            
        for file in required_files:
            if not (year_dir / file).exists():
                print(f"âœ— Missing {file} in {year_dir}")
                all_good = False
            else:
                print(f"âœ“ Found {file} in year {year}")
                
    # Check ALL_YEARS directory
    all_years_dir = data_dir / 'ALL_YEARS'
    if not all_years_dir.exists():
        print(f"âœ— ALL_YEARS directory not found")
        all_good = False
    else:
        # Note: TD files and time_series.json are generated by the model via init_ta() and print_td_data()
        # No need to check for them in ALL_YEARS
        
        # Check PES files
        pes_files = [
            'PES_seq_opti.dat',
            'PES_data_remaining.dat',
            'PES_data_all_years.dat',
            'PES_data_set_AGE_2021.dat',
            'PES_data_decom_allowed_2021.dat'
        ]
        
        for file in pes_files:
            if (all_years_dir / file).exists():
                print(f"âœ“ Found {file}")
            else:
                print(f"â“˜  Optional file not found: {file}")
                
    return all_good


def get_year_results_pathway(model, save_hourly:list=[]):
    """
    Wrapper function to get all year summary results for pathway analysis
    Calls all the pathway-specific result extraction methods
    
    These methods are now built into the EsmcPathway class directly
    """
    logging.info('Getting year summary (pathway)')
    
    # All these methods are now part of the EsmcPathway class
    model.get_total_cost_pathway()
    model.get_cost_breakdown_pathway()
    model.get_gwp_breakdown_pathway()
    model.get_resources_and_exchanges_pathway(save_hourly=save_hourly)
    model.get_assets_pathway(save_hourly=save_hourly)
    model.get_year_balance_pathway()
    model.get_curt_pathway(save_hourly=save_hourly)
    
    logging.info('All pathway results extracted successfully')
    return


def print_pathway_results(model, save_hourly:list=[]):
    """
    Print/save all pathway results to CSV files
    Similar to prints_esom but for pathway with YEARS dimension
    """
    logging.info('Printing pathway results to CSV files')
    
    directory = model.cs_dir / 'outputs'
    directory.mkdir(parents=True, exist_ok=True)
    (directory / 'regional_results').mkdir(parents=True, exist_ok=True)
    
    # Print regional results (with Years, Regions indexing)
    for key, df in model.results.items():
        if df is not None and not df.empty:
            output_file = directory / 'regional_results' / (key + '.csv')
            df.to_csv(output_file, sep=CSV_SEPARATOR)
            logging.info(f'  Saved {key}.csv to regional_results/')
            print(f"  {key}: {len(df)} rows, {len(df.columns)} columns")
    
    # Print aggregated results (with Years indexing, aggregated over regions)
    for key, df in model.results_all.items():
        if df is not None and not df.empty:
            output_file = directory / (key + '.csv')
            df.to_csv(output_file, sep=CSV_SEPARATOR)
            logging.info(f'  Saved {key}.csv to outputs/')
    
    # Print hourly results if requested
    if len(save_hourly) > 0:
        (directory / 'hourly_results').mkdir(parents=True, exist_ok=True)
        for key, df in model.hourly_results.items():
            if df is not None and not df.empty:
                output_file = directory / 'hourly_results' / (key + '.csv')
                df.to_csv(output_file, sep=CSV_SEPARATOR)
                logging.info(f'  Saved {key}.csv (hourly) to hourly_results/')
    
    logging.info('All results saved successfully')
    return


def print_results_summary(model):
    """
    Print a summary of results dimensions and values
    """
    print("\n" + "="*70)
    print("RESULTS SUMMARY")
    print("="*70)
    
    # Regional results
    print("\n[REGIONAL RESULTS]")
    for key, df in model.results.items():
        if df is not None and not df.empty:
            # Get unique values for index levels
            index_info = []
            if isinstance(df.index, pd.MultiIndex):
                for level_name in df.index.names:
                    n_unique = df.index.get_level_values(level_name).nunique()
                    index_info.append(f"{level_name}:{n_unique}")
            print(f"  {key}: {len(df)} rows | Index: {', '.join(index_info)}")
    
    # Aggregated results
    print("\n[AGGREGATED RESULTS]")
    for key, df in model.results_all.items():
        if df is not None and not df.empty:
            index_info = []
            if isinstance(df.index, pd.MultiIndex):
                for level_name in df.index.names:
                    n_unique = df.index.get_level_values(level_name).nunique()
                    index_info.append(f"{level_name}:{n_unique}")
            print(f"  {key}: {len(df)} rows | Index: {', '.join(index_info)}")
    
    print("="*70)


# ===== MAIN FUNCTION =====

def main():
    """
    Main function for pathway optimization
    """
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description='Run EnergyScope pathway optimization')
    parser.add_argument('--skip-optimize', action='store_true',
                        help='Skip optimization and load from cache (saves ~2.5 hours)')
    parser.add_argument('--force-optimize', action='store_true',
                        help='Force re-optimization even if cache exists')
    parser.add_argument('--cache-file', type=str, default=None,
                        help='Custom path for cache file')
    parser.add_argument('--skip-dat-generation', action='store_true',
                        help='Skip automatic .dat file generation')
    parser.add_argument('--skip-td-generation', action='store_true',
                        help='Skip TD file generation (use existing TD files)')
    parser.add_argument('--algo', type=str, default='read',
                        choices=['kmedoid', 'read'],
                        help='TD generation algorithm: kmedoid (generate new) or read (use existing)')
    parser.add_argument('--td-year', type=str, default='2021',
                        help='Reference year for TD generation (default: 2021)')
    
    args = parser.parse_args()
    
    # Validate arguments
    skip_optimization = args.skip_optimize
    force_optimization = args.force_optimize
    skip_dat_generation = args.skip_dat_generation
    skip_td_generation = args.skip_td_generation
    td_algo = args.algo
    td_year = args.td_year
    
    if skip_optimization and force_optimization:
        print("[ERROR] Cannot use both --skip-optimize and --force-optimize")
        return
    
    # Start timer
    start_time = time.time()
    
    print("\n" + "="*70)
    print("ENERGYSCOPE PATHWAY OPTIMIZATION")
    print("="*70)
    print(f"Case: {pathway_case}")
    print(f"Years: {years}")
    print(f"Phases: {list(phases.keys())}")
    print(f"Typical days: {nbr_tds}")
    
    if skip_optimization:
        print("\n[FAST MODE] Skipping optimization, loading from cache")
    elif force_optimization:
        print("\n[FORCE MODE] Re-optimizing even if cache exists")
    else:
        print("\n[FULL MODE] Running optimization + post-processing")
    
    print("="*70 + "\n")
    
    # ===== STEP 0: Generate .dat files if requested =====
    if not skip_dat_generation:
        print("=== Step 0: Generating .dat files for each year ===")
        print("[INFO] Generating year .dat files from source data...")
        try:
            generate_all_year_dat_files(years=years)
            print("[OK] All .dat files generated successfully\n")
        except Exception as e:
            print(f"[X] Error generating .dat files: {e}")
            print("  Continuing with existing .dat files...\n")
    else:
        print("=== Step 0: Skipping .dat file generation (--skip-dat-generation) ===\n")
    
    # Check .dat files
    print("=== Step 1: Checking .dat files ===")
    if check_dat_files():
        print("[OK] All required .dat files found\n")
    else:
        print("[X] Some .dat files are missing")
        print("  Generate them using generate_year_dat_files.py")
        print("  Or remove --skip-dat-generation flag")
        return
    
    # Create pathway model configuration
    config = {
        'case_study': pathway_case,
        'regions_names': bo_country_code,
        'gwp_limit_overall': gwp_limit_overall,
        're_share_primary': re_share_primary,
        'f_perc': f_perc,
        'space_id': '_'.join(bo_country_code)
    }
    
    years_config = {
        'years': years,
        'phases': phases,
        'pathway_params': pathway_params
    }
    
    # Initialize pathway model
    print("=== Step 2: Initializing Pathway Model ===")
    try:
        pathway_model = EsmcPathway(config, years_config, nbr_td=nbr_tds)
        print(f"[OK] Model initialized for {len(years)} years")
        print(f"  Output directory: {pathway_model.cs_dir}")
        
    except Exception as e:
        print(f"[X] Error initializing model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Check if cache exists (for info)
    cache_exists, cache_file, cache_info = check_cache_exists(pathway_model, args.cache_file)
    if cache_exists and not force_optimization:
        print(f"\n[CACHE] Found existing cache: {cache_file}")
        print(f"  Size: {cache_info.get('size_mb', 0):.1f} MB")
        print(f"  Modified: {cache_info.get('modified', 'unknown')}")
        if not skip_optimization:
            print(f"  [TIP] Use --skip-optimize to load this cache and save 2.5 hours")
    
    # Decide whether to skip optimization
    if skip_optimization:
        # Load cached results
        print("\n=== Loading Cached Results ===")
        print("[CACHE] Loading previously saved AMPL results...")
        
        # We still need to set up the model structure (but not optimize)
        try:
            # Set up the model structure (needed for result extraction)
            pathway_model.read_data_indep(year=td_year)
            pathway_model.init_regions(year=td_year)
            pathway_model.init_ta(algo=td_algo, ampl_path=ampl_path)
            pathway_model.print_td_data()
            pathway_model.process_pathway_data()
            
            print("[OK] Model structure initialized")
            
        except Exception as e:
            print(f"[X] Error setting up model: {e}")
            import traceback
            traceback.print_exc()
            return
        
        # Load cache
        success = load_ampl_results(pathway_model, cache_file=args.cache_file)
        if not success:
            print("\n[X] Failed to load cache")
            print("  Run without --skip-optimize to do full optimization")
            return
        
        print("\n[OK] Cached results loaded successfully!")
        print("  Proceeding directly to result extraction...")
        
    else:
        # Full optimization workflow
        
        # Process pathway data
        print("\n=== Step 3: Processing pathway data ===")
        print("Reading .dat files from each year and combining with year indexing...")
        try:
            # Read independent data (needed for TD generation)
            pathway_model.read_data_indep(year=td_year)
            pathway_model.init_regions(year=td_year)
            
            # Initialize temporal aggregation (TD generation)
            pathway_model.init_ta(algo=td_algo, ampl_path=ampl_path)
            
            # Print TD data to reg_16TD.dat (includes nbr_tds parameter)
            pathway_model.print_td_data()
            
            # Process all pathway data
            pathway_model.process_pathway_data()
            print("[OK] All data processed successfully")
            
            # List generated files
            dat_files = list(pathway_model.cs_dir.glob('*.dat'))
            print(f"\nGenerated {len(dat_files)} .dat files:")
            for f in sorted(dat_files)[:10]:  # Show first 10
                print(f"  - {f.name}")
            if len(dat_files) > 10:
                print(f"  ... and {len(dat_files) - 10} more files")
                
        except Exception as e:
            print(f"[X] Error processing data: {e}")
            import traceback
            traceback.print_exc()
            return
            
        # Set up AMPL model
        print("\n=== Step 4: Setting up AMPL model ===")
        try:
            pathway_model.set_esom_pathway(ampl_path=ampl_path)
            print("[OK] AMPL model configured")
            
        except Exception as e:
            print(f"[X] Error setting up AMPL: {e}")
            import traceback
            traceback.print_exc()
            return
            
        # Solve optimization
        print("\n=== Step 5: Solving optimization ===")
        print("[WAIT] This may take several hours (~2.5 hours)...")
        print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            solve_start = time.time()
            pathway_model.solve_esom_pathway()
            solve_time = time.time() - solve_start
            
            print(f"\n[OK] Optimization completed in {solve_time/60:.1f} minutes")
            
            # Check solution status
            solve_result = pathway_model.esom.ampl.getValue("solve_result")
            print(f"Solver status: {solve_result}")
            
            # Save results to cache for future runs
            print("\n=== Saving AMPL results to cache ===")
            print("[CACHE] Caching results for future fast iterations...")
            try:
                cache_file_path = save_ampl_results(
                    pathway_model, 
                    cache_file=args.cache_file
                )
                print(f"\n[OK] Results cached successfully!")
                print(f"  Next time, run with --skip-optimize to save 2.5 hours")
                print(f"  Command: python {sys.argv[0]} --skip-optimize")
            except Exception as e:
                print(f"[WARN] Warning: Could not cache results: {e}")
                print("  Continuing with result extraction...")
            
        except Exception as e:
            print(f"[X] Error during optimization: {e}")
            import traceback
            traceback.print_exc()
            return
    
    # From here on, same workflow for both modes
    # (both have AMPL results available - either fresh or cached)
    
    # Extract New/Old/Decom results (keeping existing functionality)
    print("\n=== Step 6: Processing New/Old/Decom Results ===")
    try:
        # Output directory
        output_dir = pathway_model.cs_dir / 'outputs'
        
        # Collect data using pathway_model method
        new_old_decom_df = pathway_model.collect_new_old_decom(output_dir=output_dir)
        
        if new_old_decom_df is not None:
            # Generate plots using pathway_model method
            df_results = pathway_model.plot_new_old_decom_by_sector(
                new_old_decom_df=new_old_decom_df,
                output_dir=output_dir
            )
            
            print(f"\n[OK] New/Old/Decom results saved")
            print(f"  - CSV: outputs/New_old_decom.csv")
            print(f"  - Full data: outputs/New_old_decom_graphs/New_old_decom_full_data.csv")
            print(f"  - Plots: outputs/New_old_decom_graphs/")
        
    except Exception as e:
        print(f"[X] Error processing New/Old/Decom: {e}")
        import traceback
        traceback.print_exc()
    
    # Extract ALL pathway results with YEARS dimension
    print("\n=== Step 7: Extracting Pathway Results (with YEARS) ===")
    try:
        extraction_start = time.time()
        
        # Call wrapper function to extract all results
        get_year_results_pathway(pathway_model, save_hourly=save_hourly)
        
        extraction_time = time.time() - extraction_start
        print(f"[OK] All results extracted in {extraction_time:.1f} seconds")
        
    except Exception as e:
        print(f"[X] Error extracting pathway results: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Save all results to CSV files
    print("\n=== Step 8: Saving Results to CSV ===")
    try:
        save_start = time.time()
        
        print_pathway_results(pathway_model, save_hourly=save_hourly)
        
        save_time = time.time() - save_start
        print(f"[OK] All CSV files saved in {save_time:.1f} seconds")
        
    except Exception as e:
        print(f"[X] Error saving results: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Generate cost plots (NEW STEP)
    print("\n=== Step 9: Generating Cost Plots ===")
    try:
        plot_start = time.time()
        
        # Investment cost plots
        print("  Generating investment cost plots...")
        df_inv = pathway_model.graph_cost_inv_phase_tech(plot=True)
        if df_inv is not None:
            print(f"  [OK] Investment plots saved to outputs/Cost_Inv_Phase/")
        
        # Operation cost plots
        print("  Generating operation cost plots...")
        df_op = pathway_model.graph_cost_op_phase(plot=True)
        if df_op is not None:
            print(f"  [OK] Operation plots saved to outputs/Cost_Op_Phase/")
        
        plot_time = time.time() - plot_start
        print(f"[OK] All cost plots generated in {plot_time:.1f} seconds")
        
    except Exception as e:
        print(f"[X] Error generating cost plots: {e}")
        import traceback
        traceback.print_exc()
    
    # Print summary of results
    print("\n=== Step 10: Results Summary ===")
    try:
        print_results_summary(pathway_model)
    except Exception as e:
        print(f"[INFO] Error printing summary: {e}")
    
    # List all generated CSV files
    print("\n=== Generated CSV Files ===")
    try:
        output_dir = pathway_model.cs_dir / 'outputs'
        
        print("\nRegional Results (with Years and Regions):")
        regional_dir = output_dir / 'regional_results'
        if regional_dir.exists():
            regional_files = sorted(regional_dir.glob('*.csv'))
            for f in regional_files:
                print(f"  [OK] {f.name}")
        
        print("\nAggregated Results (with Years, aggregated over Regions):")
        agg_files = [f for f in sorted(output_dir.glob('*.csv'))]
        for f in agg_files:
            print(f"  [OK] {f.name}")
        
        print("\nHourly Results (if requested):")
        hourly_dir = output_dir / 'hourly_results'
        if hourly_dir.exists():
            hourly_files = sorted(hourly_dir.glob('*.csv'))
            for f in hourly_files:
                print(f"  [OK] {f.name}")
        
        print("\nCost Plots:")
        cost_inv_dir = output_dir / 'Cost_Inv_Phase'
        if cost_inv_dir.exists():
            print(f"  [OK] Investment plots in Cost_Inv_Phase/")
        cost_op_dir = output_dir / 'Cost_Op_Phase'
        if cost_op_dir.exists():
            print(f"  [OK] Operation plots in Cost_Op_Phase/")
        
    except Exception as e:
        print(f"[INFO] Error listing files: {e}")
    
    # Clean up
    try:
        pathway_model.esom.ampl.close()
        print("\n[OK] AMPL resources cleaned up")
    except:
        pass
        
    # Total time
    total_time = time.time() - start_time
    print(f"\n{'='*70}")
    if skip_optimization:
        print(f"PATHWAY POST-PROCESSING COMPLETED (Fast Mode)")
        print(f"[FAST] Saved ~2.5 hours by using cached results!")
    else:
        print(f"PATHWAY OPTIMIZATION COMPLETED")
    print(f"{'='*70}")
    print(f"Total execution time: {total_time/60:.1f} minutes")
    print(f"Results directory: {pathway_model.cs_dir / 'outputs'}")
    
    if not skip_optimization:
        print(f"\n[TIP] Next time, use --skip-optimize to save 2.5 hours:")
        print(f"   python {sys.argv[0]} --skip-optimize")
    
    if not skip_td_generation and td_algo == 'read':
        print(f"\n[TIP] To regenerate TD files, use:")
        print(f"   python {sys.argv[0]} --algo kmedoid --td-year {td_year}")
    
    print(f"\nAll CSV files include YEARS dimension:")
    print(f"  - TotalCost.csv")
    print(f"  - Cost_breakdown.csv")
    print(f"  - Gwp_breakdown.csv")
    print(f"  - Exchanges_year.csv")
    print(f"  - Transfer_capacity.csv")
    print(f"  - Resources.csv")
    print(f"  - Assets.csv")
    print(f"  - Sto_assets.csv")
    print(f"  - Year_balance.csv")
    print(f"  - Curt.csv")
    if save_hourly:
        print(f"  - Hourly results for: {', '.join(save_hourly)}")
    print(f"\nCost plots:")
    print(f"  - Investment: Cost_Inv_Phase/cost_inv_phase_tech.html & .pdf")
    print(f"  - Operation: Cost_Op_Phase/cost_op_phase.html & .pdf")
    print(f"\n{'='*70}")


if __name__ == "__main__":
    main()